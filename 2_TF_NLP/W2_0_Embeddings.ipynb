{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings & Sequence Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 10:01:26.636161: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/chiraj/iWork/TensorFlow/tensorflow-cert/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization & Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in word_index 7\n",
      "word_index {'<OOV>': 1, 'is': 2, 'ml': 3, 'good': 4, 'dl': 5, 'awesome': 6, 'explainable': 7}\n",
      "\n",
      "sequences [[3, 2, 4], [5, 2, 6], [3, 2, 7]]\n",
      "padded [[3 2 4]\n",
      " [5 2 6]\n",
      " [3 2 7]]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'ML is good',\n",
    "    'DL is awesome',\n",
    "    'ML is explainable'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print('number of words in word_index', len(word_index))\n",
    "\n",
    "\n",
    "print('word_index', word_index)\n",
    "print()\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "\n",
    "print('sequences', sequences)\n",
    "print('padded', padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Review Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb, info = tfds.load(\n",
    "    'imdb_reviews',\n",
    "    with_info=True,\n",
    "    as_supervised=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'train': <_PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>,\n",
       "  'test': <_PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>,\n",
       "  'unsupervised': <_PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>},\n",
       " '--------------------------------------------------',\n",
       " tfds.core.DatasetInfo(\n",
       "     name='imdb_reviews',\n",
       "     full_name='imdb_reviews/plain_text/1.0.0',\n",
       "     description=\"\"\"\n",
       "     Large Movie Review Dataset. This is a dataset for binary sentiment\n",
       "     classification containing substantially more data than previous benchmark\n",
       "     datasets. We provide a set of 25,000 highly polar movie reviews for training,\n",
       "     and 25,000 for testing. There is additional unlabeled data for use as well.\n",
       "     \"\"\",\n",
       "     config_description=\"\"\"\n",
       "     Plain text\n",
       "     \"\"\",\n",
       "     homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
       "     data_dir='/Users/chiraj/tensorflow_datasets/imdb_reviews/plain_text/1.0.0',\n",
       "     file_format=tfrecord,\n",
       "     download_size=80.23 MiB,\n",
       "     dataset_size=129.83 MiB,\n",
       "     features=FeaturesDict({\n",
       "         'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
       "         'text': Text(shape=(), dtype=string),\n",
       "     }),\n",
       "     supervised_keys=('text', 'label'),\n",
       "     disable_shuffling=False,\n",
       "     splits={\n",
       "         'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
       "         'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
       "         'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
       "     },\n",
       "     citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
       "       author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
       "       title     = {Learning Word Vectors for Sentiment Analysis},\n",
       "       booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
       "       month     = {June},\n",
       "       year      = {2011},\n",
       "       address   = {Portland, Oregon, USA},\n",
       "       publisher = {Association for Computational Linguistics},\n",
       "       pages     = {142--150},\n",
       "       url       = {http://www.aclweb.org/anthology/P11-1015}\n",
       "     }\"\"\",\n",
       " ))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb, '-'*50, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = imdb['train'], imdb['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven't laughed this hard since I saw THE FULL MONTY. (And, even then, I don't think I laughed quite this hard... So to speak.) Tukel's talent is considerable: DING-A-LING-LESS is so chock full of double entendres that one would have to sit down with a copy of this script and do a line-by-line examination of it to fully appreciate the, uh, breadth and width of it. Every shot is beautifully composed (a clear sign of a sure-handed director), and the performances all around are solid (there's none of the over-the-top scenery chewing one might've expected from a film like this). DING-A-LING-LESS is a film whose time has come.\">, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      "--------------------------------------------------\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"A blackly comic tale of a down-trodden priest, Nazarin showcases the economy that Luis Bunuel was able to achieve in being able to tell a deeply humanist fable with a minimum of fuss. As an output from his Mexican era of film making, it was an invaluable talent to possess, with little money and extremely tight schedules. Nazarin, however, surpasses many of Bunuel's previous Mexican films in terms of the acting (Francisco Rabal is excellent), narrative and theme.<br /><br />The theme, interestingly, is something that was explored again in Viridiana, made three years later in Spain. It concerns the individual's struggle for humanity and altruism amongst a society that rejects any notion of virtue. Father Nazarin, however, is portrayed more sympathetically than Sister Viridiana. Whereas the latter seems to choose charity because she wishes to atone for her (perceived) sins, Nazarin's whole existence and reason for being seems to be to help others, whether they (or we) like it or not. The film's last scenes, in which he casts doubt on his behaviour and, in a split second, has to choose between the life he has been leading or the conventional life that is expected of a priest, are so emotional because they concern his moral integrity and we are never quite sure whether it remains intact or not.<br /><br />This is a remarkable film and I would urge anyone interested in classic cinema to seek it out. It is one of Bunuel's most moving films, and encapsulates many of his obsessions: frustrated desire, mad love, religious hypocrisy etc. In my view 'Nazarin' is second only to 'The Exterminating Angel', in terms of his Mexican movies, and is certainly near the top of the list of Bunuel's total filmic output.\">, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 10:01:42.705837: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-10-31 10:01:42.706304: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for train_example, test_example in zip(train_data.take(2), test_data.take(2)):\n",
    "    print(train_example)\n",
    "    print(test_example)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_raw_data(data):\n",
    "\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    for s,l in data:\n",
    "        sentences.append(s.numpy().decode('utf8'))\n",
    "        labels.append(l.numpy())\n",
    "    \n",
    "    labels_final = np.array(labels)\n",
    "\n",
    "    return (sentences, labels_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences, training_labels = transform_raw_data(train_data)\n",
    "testing_sentences, testing_labels = transform_raw_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\",\n",
       "  'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.',\n",
       "  'Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? <br /><br />Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.<br /><br />Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust.',\n",
       "  'This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful performances from Cher and Nicolas Cage (as always) gently row the plot along. There are no rapids to cross, no dangerous waters, just a warm and witty paddle through New York life at its best. A family film in every sense and one that deserves the praise it received.',\n",
       "  'As others have mentioned, all the women that go nude in this film are mostly absolutely gorgeous. The plot very ably shows the hypocrisy of the female libido. When men are around they want to be pursued, but when no \"men\" are around, they become the pursuers of a 14 year old boy. And the boy becomes a man really fast (we should all be so lucky at this age!). He then gets up the courage to pursue his true love.'],\n",
       " array([0, 0, 0, 1, 1]),\n",
       " [\"There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven't laughed this hard since I saw THE FULL MONTY. (And, even then, I don't think I laughed quite this hard... So to speak.) Tukel's talent is considerable: DING-A-LING-LESS is so chock full of double entendres that one would have to sit down with a copy of this script and do a line-by-line examination of it to fully appreciate the, uh, breadth and width of it. Every shot is beautifully composed (a clear sign of a sure-handed director), and the performances all around are solid (there's none of the over-the-top scenery chewing one might've expected from a film like this). DING-A-LING-LESS is a film whose time has come.\",\n",
       "  \"A blackly comic tale of a down-trodden priest, Nazarin showcases the economy that Luis Bunuel was able to achieve in being able to tell a deeply humanist fable with a minimum of fuss. As an output from his Mexican era of film making, it was an invaluable talent to possess, with little money and extremely tight schedules. Nazarin, however, surpasses many of Bunuel's previous Mexican films in terms of the acting (Francisco Rabal is excellent), narrative and theme.<br /><br />The theme, interestingly, is something that was explored again in Viridiana, made three years later in Spain. It concerns the individual's struggle for humanity and altruism amongst a society that rejects any notion of virtue. Father Nazarin, however, is portrayed more sympathetically than Sister Viridiana. Whereas the latter seems to choose charity because she wishes to atone for her (perceived) sins, Nazarin's whole existence and reason for being seems to be to help others, whether they (or we) like it or not. The film's last scenes, in which he casts doubt on his behaviour and, in a split second, has to choose between the life he has been leading or the conventional life that is expected of a priest, are so emotional because they concern his moral integrity and we are never quite sure whether it remains intact or not.<br /><br />This is a remarkable film and I would urge anyone interested in classic cinema to seek it out. It is one of Bunuel's most moving films, and encapsulates many of his obsessions: frustrated desire, mad love, religious hypocrisy etc. In my view 'Nazarin' is second only to 'The Exterminating Angel', in terms of his Mexican movies, and is certainly near the top of the list of Bunuel's total filmic output.\",\n",
       "  'Scary Movie 1-4, Epic Movie, Date Movie, Meet the Spartans, Not another Teen Movie and Another Gay Movie. Making \"Superhero Movie\" the eleventh in a series that single handily ruined the parody genre. Now I\\'ll admit it I have a soft spot for classics such as Airplane and The Naked Gun but you know you\\'ve milked a franchise so bad when you can see the gags a mile off. In fact the only thing that might really temp you into going to see this disaster is the incredibly funny but massive sell-out Leslie Neilson.<br /><br />You can tell he needs the money, wither that or he intends to go down with the ship like a good Capitan would. In no way is he bringing down this genre but hell he\\'s not helping it. But if I feel sorry for anybody in this film its decent actor Drake Bell who is put through an immense amount of embarrassment. The people who are put through the largest amount of torture by far however is the audience forced to sit through 90 minutes of laughless bile no funnier than herpes.<br /><br />After spoofing disaster films in Airplane!, police shows in The Naked Gun, and Hollywood horrors in Scary Movie 3 and 4, producer David Zucker sets his satirical sights on the superhero genre with this anarchic comedy lampooning everything from Spider-Man to X-Men and Superman Returns.<br /><br />Shortly after being bitten by a genetically altered dragonfly, high-school outcast Rick Riker (Drake Bell) begins to experience a startling transformation. Now Rick\\'s skin is as strong as steel, and he possesses the strength of ten men. Determined to use his newfound powers to fight crime, Rick creates a special costume and assumes the identity of The Dragonfly -- a fearless crime fighter dedicated to keeping the streets safe for law-abiding citizens.<br /><br />But every superhero needs a nemesis, and after Lou Landers (Christopher McDonald) is caught in the middle of an experiment gone horribly awry, he develops the power to leech the life force out of anyone he meets and becomes the villainous Hourglass. Intent on achieving immortality, the Hourglass attempts to gather as much life force as possible as the noble Dragonfly sets out to take down his archenemy and realize his destiny as a true hero. Craig Mazin writes and directs this low-flying spoof.<br /><br />featuring Tracy Morgan, Pamela Anderson, Leslie Nielsen, Marion Ross, Jeffrey Tambor, and Regina Hall.<br /><br />Hell Superhero Movie may earn some merit in the fact that it\\'s a hell of a lot better than Meet the Spartans and Epic Movie. But with great responsibility comes one of the worst outings of 2008 to date. Laughless but a little less irritating than Meet the Spartans. And in the same sense much more forgettable than meet the Spartans. But maybe that\\'s a good reason. There are still some of us trying to scrape away the stain that was Meet the Spartans from our memory.<br /><br />My final verdict? Avoid, unless you\\'re one of thoses people who enjoy such car crash cinema. As bad as Date Movie and Scary Movie 2 but not quite as bad as Meet the Spartans or Epic Movie. Super Villain.',\n",
       "  'Poor Shirley MacLaine tries hard to lend some gravitas to this mawkish, gag-inducing \"feel-good\" movie, but she\\'s trampled by the run-away sentimentality of a film that\\'s not the least bit grounded in reality.<br /><br />This was directed by Curtis Hanson? Did he have a lobotomy since we last heard from him? Hanson can do effective drama sprinkled with comedy, as evidenced by \"Wonder Boys.\" So I don\\'t know what happened to him here. This is the kind of movie that doesn\\'t want to accept that life is messy and fussy, and that neat, tidy endings (however implausible they might be) might make for a nice closing shot, but come across as utterly phony if the people watching the film have been through anything remotely like what the characters in the film go through.<br /><br />My wife and I made a game of calling out the plot points before they occurred -- e.g. \"the old man\\'s going to teach her to read and then drop dead.\" Bingo! This is one of those movies where the characters give little speeches summarizing their emotional problems, making you wonder why they still have emotional problems if they\\'re that aware of what\\'s causing them. Toni Collette (a fine actress, by the way, and one of my favorites if not given a lot to work with here), gives a speech early on about why she buys so many shoes and never wears them, spelling out in flashing neon the film\\'s awkward connecting motif. At that moment, I knew what I was in for, and the film was a downward spiral from there.<br /><br />Grade: C-',\n",
       "  'As a former Erasmus student I enjoyed this film very much. It was so realistic and funny. It really picked up the spirit that exists among Erasmus students. I hope, many other students will follow this experience, too. However, I wonder if this movie is all that interesting to watch for people with no international experience. But at least one of my friends who has never gone on Erasmus also enjoyed it very much. I give it 9 out of 10.'],\n",
       " array([1, 1, 0, 0, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sentences[:5], training_labels[:5], testing_sentences[:5], testing_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_text(train_sentences, test_sentences,\n",
    "                     vocab_size=10000, max_len=120, trunc_type='post', oov_token='<OOV>'):\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
    "    tokenizer.fit_on_texts(train_sentences)\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "    train_padded = pad_sequences(train_sequences, maxlen=max_len, truncating=trunc_type)\n",
    "\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "    test_padded = pad_sequences(test_sequences, maxlen=max_len, truncating=trunc_type)\n",
    "\n",
    "    return tokenizer, train_padded, test_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=10000\n",
    "embedding_dim=16\n",
    "max_len=120\n",
    "trunc_type='post'\n",
    "oov_token='<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, training_padded, testing_padded = pre_process_text(training_sentences, testing_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    0,    0,   12,   14,   33,  425,  392,   18,   90,   28,\n",
       "           1,    9,   32, 1366, 3585,   40,  486,    1,  197,   24,   85,\n",
       "         154,   19,   12,  213,  329,   28,   66,  247,  215,    9,  477,\n",
       "          58,   66,   85,  114,   98,   22, 5675,   12, 1322,  643,  767,\n",
       "          12,   18,    7,   33,  400, 8170,  176, 2455,  416,    2,   89,\n",
       "        1231,  137,   69,  146,   52,    2,    1, 7577,   69,  229,   66,\n",
       "        2933,   16,    1, 2904,    1,    1, 1479, 4940,    3,   39, 3900,\n",
       "         117, 1584,   17, 3585,   14,  162,   19,    4, 1231,  917, 7917,\n",
       "           9,    4,   18,   13,   14, 4139,    5,   99,  145, 1214,   11,\n",
       "         242,  683,   13,   48,   24,  100,   38,   12, 7181, 5515,   38,\n",
       "        1366,    1,   50,  401,   11,   98, 1197,  867,  141,   10],\n",
       "       dtype=int32),\n",
       " array([  48,   24,  106,   13,   95, 4066,   16,  740, 5065,   10,   14,\n",
       "         312,    5,    2,  579,  349,   16, 1847, 1257,    1,   16,  668,\n",
       "        7666, 5531,    1,  761,    6,   13, 1026,    1,    1,  425,  478,\n",
       "           1,    4,    1,  327, 3560,   20,  229,    3,   15, 5742,    3,\n",
       "          15, 1620,   15,   99,    5,    2, 3550,  100,   11,  772, 1498,\n",
       "          12,  252,  235,   11,  217,    2,  366, 6454,    3,   58,   93,\n",
       "          11,   90,  102,   11, 1498,  177,   12,  252,   36,    6, 1126,\n",
       "           1,  674,    7, 4387,    1,    4,    1,  327,    7,   36, 8300,\n",
       "         366,    5, 1403,    1,   13,   29,   60,   26,    6,  867,  178,\n",
       "          17,    4, 1037,    5,   12,  227,    3,   79,    4,  345,   32,\n",
       "         345, 5159,    5,   10,    6, 1314, 1143,    2, 5619,    1],\n",
       "       dtype=int32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_padded[0], testing_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(6, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 120, 16)           160000    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1920)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 11526     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 171533 (670.05 KB)\n",
      "Trainable params: 171533 (670.05 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.4960 - accuracy: 0.7395 - val_loss: 0.3812 - val_accuracy: 0.8282\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2310 - accuracy: 0.9136 - val_loss: 0.4289 - val_accuracy: 0.8120\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0832 - accuracy: 0.9789 - val_loss: 0.5020 - val_accuracy: 0.8098\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0194 - accuracy: 0.9977 - val_loss: 0.6043 - val_accuracy: 0.8059\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0045 - accuracy: 0.9998 - val_loss: 0.6704 - val_accuracy: 0.8091\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.7263 - val_accuracy: 0.8097\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 8.0554e-04 - accuracy: 1.0000 - val_loss: 0.7751 - val_accuracy: 0.8105\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 4.4220e-04 - accuracy: 1.0000 - val_loss: 0.8201 - val_accuracy: 0.8100\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 2.5438e-04 - accuracy: 1.0000 - val_loss: 0.8634 - val_accuracy: 0.8096\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 1.6372e-04 - accuracy: 1.0000 - val_loss: 0.9048 - val_accuracy: 0.8102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x144ee73d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "model.fit(\n",
    "    training_padded,\n",
    "    training_labels,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=(testing_padded, testing_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viz Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.src.layers.core.embedding.Embedding at 0x144ca3250>,\n",
       " <keras.src.layers.reshaping.flatten.Flatten at 0x144f1b850>,\n",
       " <keras.src.layers.core.dense.Dense at 0x144ca3c10>,\n",
       " <keras.src.layers.core.dense.Dense at 0x145b63990>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_weights = model.layers[0].get_weights()[0]\n",
    "\n",
    "embedding_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = tokenizer.index_word\n",
    "\n",
    "# Open writeable files\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "# Initialize the loop. Start counting at `1` because `0` is just for the padding\n",
    "for word_num in range(1, vocab_size):\n",
    "\n",
    "  # Get the word associated at the current index\n",
    "  word_name = reverse_word_index[word_num]\n",
    "\n",
    "  # Get the embedding weights associated with the current index\n",
    "  word_embedding = embedding_weights[word_num]\n",
    "\n",
    "  # Write the word name\n",
    "  out_m.write(word_name + \"\\n\")\n",
    "\n",
    "  # Write the word embedding\n",
    "  out_v.write('\\t'.join([str(x) for x in word_embedding]) + \"\\n\")\n",
    "\n",
    "# Close the files\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the tsv files to [visualize](https://projector.tensorflow.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
